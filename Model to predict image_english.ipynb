{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_SIZE = 32\n",
    "\n",
    "\n",
    "def list_folders(root_folder):\n",
    "    \"\"\"Function to get subdir list\"\"\"\n",
    "    folder_list = []\n",
    "    for folder in sorted(os.listdir(root_folder)):\n",
    "        if os.path.isdir(os.path.join(root_folder, folder)):\n",
    "            folder_list.append(folder)\n",
    "    return folder_list\n",
    "\n",
    "\n",
    "def create_folders(root_folder, folder_list):\n",
    "    \"\"\"Function to create folders in new dataset\"\"\"\n",
    "    for folder in folder_list:\n",
    "        os.makedirs(os.path.join(root_folder, folder), exist_ok=True)\n",
    "\n",
    "\n",
    "def read_transparent_png(filename):\n",
    "    \"\"\"\n",
    "    Change transparent bg to white\n",
    "    \"\"\"\n",
    "    image_4channel = cv2.imread(filename, cv2.IMREAD_UNCHANGED)\n",
    "    alpha_channel = image_4channel[:, :, 0]\n",
    "    rgb_channels = image_4channel[:, :, :3]\n",
    "\n",
    "    # White Background Image\n",
    "    white_background_image = np.ones_like(rgb_channels, dtype=np.uint8) * 10\n",
    "\n",
    "    # Alpha factor\n",
    "    alpha_factor = alpha_channel[:, :, np.newaxis].astype(np.float32) / 255.0\n",
    "    alpha_factor = np.concatenate((alpha_factor, alpha_factor, alpha_factor), axis=2)\n",
    "\n",
    "    # Transparent Image Rendered on White Background\n",
    "    base = rgb_channels.astype(np.float32) * alpha_factor\n",
    "    white = white_background_image.astype(np.float32) * (1 - alpha_factor)\n",
    "    final_image = base + white\n",
    "    return final_image.astype(np.uint8)\n",
    "\n",
    "\n",
    "def clean(img):\n",
    "    \"\"\"Process an image\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    #(__, img_bw) = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "\n",
    "    #ctrs, __ = cv2.findContours(img_bw.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # take largest contour\n",
    "    #ctr = sorted(ctrs, key=lambda ctr: (cv2.boundingRect(ctr)[2] * cv2.boundingRect(ctr)[3]),\n",
    "                 #reverse=True)[0]\n",
    "    # Get bounding box\n",
    "    #x, y, w, h = cv2.boundingRect(ctr)\n",
    "\n",
    "    # Getting ROI\n",
    "    #roi = img_bw[y:y + h, x:x + w]\n",
    "    return crop(gray, IMAGE_SIZE)\n",
    "\n",
    "\n",
    "def crop(image, desired_size):\n",
    "    \"\"\"Crop and pad to req size\"\"\"\n",
    "    #old_size = image.shape[:2]  # old_size is in (height, width) format\n",
    "    #ratio = float(desired_size) / max(old_size)\n",
    "    #new_size = tuple([int(x * ratio) for x in old_size])\n",
    "\n",
    "    # new_size should be in (width, height) format\n",
    "    im = cv2.resize(image, (32, 32))\n",
    "\n",
    "    #delta_w = desired_size - new_size[1]\n",
    "    #delta_h = desired_size - new_size[0]\n",
    "    #top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    #left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "    #color = [0, 0, 0]\n",
    "    #new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "                                #value=color)\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def process_folder(folder):\n",
    "    \"\"\"Process all images in a folder\"\"\"\n",
    "    extension = '.png'\n",
    "    new_list = []\n",
    "    for img in sorted(os.listdir(folder)):\n",
    "        if img.endswith(extension):\n",
    "            image = read_transparent_png(os.path.join(folder, img))\n",
    "            new_img = clean(image)\n",
    "            new_list.append([img, new_img])\n",
    "            \"\"\"except:\n",
    "                print(\"\\t\" + img)\"\"\"\n",
    "    return new_list\n",
    "\n",
    "\n",
    "def save_new(folder, imglist):\n",
    "    \"\"\"Save newly created images\"\"\"\n",
    "    for img in imglist:\n",
    "        cv2.imwrite(os.path.join(folder, img[0]), img[1])\n",
    "\n",
    "\n",
    "def process_images(raw_folder, clean_folder, folder_list):\n",
    "    \"\"\"Process the images\"\"\"\n",
    "    for folder in folder_list:\n",
    "        print(folder)\n",
    "        imglist = process_folder(os.path.join(raw_folder, folder, 'output'))\n",
    "        save_new(os.path.join(clean_folder, folder), imglist)\n",
    "\n",
    "\n",
    "def skeletize(img):\n",
    "    size = np.size(img)\n",
    "    skel = np.zeros(img.shape, np.uint8)\n",
    "    element = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        eroded = cv2.erode(img, element)\n",
    "        temp = cv2.dilate(eroded, element)\n",
    "        temp = cv2.subtract(img, temp)\n",
    "        skel = cv2.bitwise_or(skel, temp)\n",
    "        img = eroded.copy()\n",
    "\n",
    "        zeroes = size - cv2.countNonZero(img)\n",
    "        if zeroes == size:\n",
    "            done = True\n",
    "\n",
    "    return skel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\n",
      "3\n",
      "to\n",
      "You\n",
      "have\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = \"C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "words=[]\n",
    "a=['1','2','3']\n",
    "for i in range(3):\n",
    "    english=pytesseract.image_to_string(Image.open(\"D:\\\\Eng_dataset\\\\\"+a[i]+\"\\\\\"+a[i]+\".png\"),lang=\"eng\")\n",
    "    #print(hindi)\n",
    "    \n",
    "    words.append(english)\n",
    "\n",
    "img=cv2.imread(\"D:\\\\Eng_dataset\\\\2\\\\2.png\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "hindi=pytesseract.image_to_string(Image.open(\"D:\\\\Eng_dataset\\\\2\\\\2.png\"),lang=\"eng\")\n",
    "print(english)\n",
    "\n",
    "print(len(words))\n",
    "print(words[2])\n",
    "\n",
    "for i in range(3):\n",
    "    print(words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1.png:   0%|▏                                                        | 3/1000 [00:00<00:37, 26.26 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Eng_dataset\\1\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1.png: 100%|██████████████████████████████████████████████████████| 1000/1000 [00:28<00:00, 35.06 Samples/s]\n",
      "Processing 2.png:   0%|▎                                                        | 5/1000 [00:00<00:26, 37.38 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Eng_dataset\\2\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2.png: 100%|██████████████████████████████████████████████████████| 1000/1000 [00:34<00:00, 28.69 Samples/s]\n",
      "Processing 3.png:   1%|▌                                                       | 10/1000 [00:00<00:16, 61.46 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Eng_dataset\\3\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 3.png: 100%|██████████████████████████████████████████████████████| 1000/1000 [00:18<00:00, 52.75 Samples/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import Augmentor\n",
    "\n",
    "\n",
    "folder = \"D:\\\\Eng_dataset\"\n",
    "for f in list_folders(folder):\n",
    "    if os.path.isdir(os.path.join(folder, f, 'output')):\n",
    "        shutil.rmtree(os.path.join(folder, f, 'output'))\n",
    "    p = Augmentor.Pipeline(os.path.join(folder, f))\n",
    "    p.random_distortion(probability=1, grid_width=10, grid_height=10, magnitude=8)\n",
    "    p.sample(1000, multi_threaded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3']\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "RAW_FOLDER = \"D:\\\\Eng_dataset\"\n",
    "CLEAN_FOLDER = \"D:\\\\Eng_dataset\\\\clean\"\n",
    "\n",
    "FOLDER_LIST = list_folders(RAW_FOLDER)\n",
    "print(FOLDER_LIST)\n",
    "\n",
    "create_folders(CLEAN_FOLDER, FOLDER_LIST)\n",
    "process_images(RAW_FOLDER, CLEAN_FOLDER, FOLDER_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Eng_dataset\\clean\\1\n",
      "Full dataset tensor: (1000, 32, 32)\n",
      "Mean: 0.84204394\n",
      "Standard deviation: 0.3646998\n",
      "D:\\Eng_dataset\\clean\\2\n",
      "Full dataset tensor: (1000, 32, 32)\n",
      "Mean: 0.856374\n",
      "Standard deviation: 0.35071003\n",
      "D:\\Eng_dataset\\clean\\3\n",
      "Full dataset tensor: (1000, 32, 32)\n",
      "Mean: 0.8898955\n",
      "Standard deviation: 0.3130199\n",
      "3\n",
      "100 200 700\n",
      "0 100\n",
      "100 300\n",
      "300 1000\n",
      "Training set (2100, 32, 32) (2100,)\n",
      "Test set (600, 32, 32) (600,)\n",
      "Validation set (300, 32, 32) (300,)\n",
      "Compressed pickle size: 12300493\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from six.moves import cPickle as Pickle\n",
    "import csv\n",
    "\n",
    "DATA_FOLDER = \"D:\\\\Eng_dataset\\\\clean\"\n",
    "image_size = 32\n",
    "pixel_depth = 255\n",
    "pickle_extension = '.pickle'\n",
    "num_classes = 3\n",
    "image_per_class = 1000\n",
    "\n",
    "\n",
    "def get_folders(path):\n",
    "    data_folders = [os.path.join(path, d) for d in sorted(os.listdir(path))\n",
    "                    if os.path.isdir(os.path.join(path, d))]\n",
    "\n",
    "    if len(data_folders) != num_classes:\n",
    "        raise Exception(\n",
    "            'Expected %d folders, one per class. Found %d instead.' % (\n",
    "                num_classes, len(data_folders)))\n",
    "\n",
    "    return data_folders\n",
    "\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    \"\"\"Load the data for a single letter label.\"\"\"\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "    print(folder)\n",
    "    image_index = -1\n",
    "    for image_index, image in enumerate(image_files):\n",
    "        image_file = os.path.join(folder, image)\n",
    "        try:\n",
    "            image_data = 1 * (cv2.imread(image_file, cv2.IMREAD_UNCHANGED).astype(float) > pixel_depth / 2)\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            dataset[image_index, :, :] = image_data\n",
    "        except IOError as err:\n",
    "            print('Could not read:', image_file, ':', err, '- it\\'s ok, skipping.')\n",
    "\n",
    "    num_images = image_index + 1\n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' % (num_images, min_num_images))\n",
    "\n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Standard deviation:', np.std(dataset))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + pickle_extension\n",
    "        dataset_names.append(folder)\n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            # You may override by setting force=True.\n",
    "            print('%s already present - Skipping pickling.' % set_filename)\n",
    "        else:\n",
    "            # print('Pickling %s.' % set_filename)\n",
    "            dataset = load_letter(folder, min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    Pickle.dump(dataset, f, Pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':', e)\n",
    "\n",
    "    return dataset_names\n",
    "\n",
    "\n",
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "        labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, test_size=0, valid_size=0):\n",
    "    num_classes = len(pickle_files)\n",
    "    print(num_classes)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "    test_dataset, test_labels = make_arrays(test_size, image_size)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "    valid_size_per_class = valid_size // num_classes\n",
    "    test_size_per_class = test_size // num_classes\n",
    "    train_size_per_class = train_size // num_classes\n",
    "\n",
    "    print(valid_size_per_class, test_size_per_class, train_size_per_class)\n",
    "\n",
    "    start_valid, start_test, start_train = 0, valid_size_per_class, (valid_size_per_class + test_size_per_class)\n",
    "    end_valid = valid_size_per_class\n",
    "    end_test = end_valid + test_size_per_class\n",
    "    end_train = end_test + train_size_per_class\n",
    "\n",
    "    print(start_valid, end_valid)\n",
    "    print(start_test, end_test)\n",
    "    print(start_train,end_train)\n",
    "\n",
    "    s_valid, s_test, s_train = 0, 0, 0\n",
    "    e_valid, e_test, e_train = valid_size_per_class, test_size_per_class, train_size_per_class\n",
    "    temp = []\n",
    "    for label, pickle_file in enumerate(pickle_files):\n",
    "        temp.append([label, pickle_file[-4:]])\n",
    "        try:\n",
    "            with open(pickle_file + pickle_extension, 'rb') as f:\n",
    "                letter_set = Pickle.load(f)\n",
    "                # let's shuffle the letters to have random validation and training set\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter = letter_set[:end_valid, :, :]\n",
    "                    valid_dataset[s_valid:e_valid, :, :] = valid_letter\n",
    "                    valid_labels[s_valid:e_valid] = label\n",
    "                    s_valid += valid_size_per_class\n",
    "                    e_valid += valid_size_per_class\n",
    "\n",
    "                if test_dataset is not None:\n",
    "                    test_letter = letter_set[start_test:end_test, :, :]\n",
    "                    test_dataset[s_test:e_test, :, :] = test_letter\n",
    "                    test_labels[s_test:e_test] = label\n",
    "                    s_test += test_size_per_class\n",
    "                    e_test += test_size_per_class\n",
    "\n",
    "                train_letter = letter_set[start_train:end_train, :, :]\n",
    "                train_dataset[s_train:e_train, :, :] = train_letter\n",
    "                train_labels[s_train:e_train] = label\n",
    "                s_train += train_size_per_class\n",
    "                e_train += train_size_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':', e)\n",
    "            raise\n",
    "    with open('classes.csv', 'w') as my_csv:\n",
    "        writer = csv.writer(my_csv, delimiter=',')\n",
    "        writer.writerows(temp)\n",
    "    return valid_dataset, valid_labels, test_dataset, test_labels, train_dataset, train_labels\n",
    "\n",
    "\n",
    "data_folders = get_folders(DATA_FOLDER)\n",
    "train_datasets = maybe_pickle(data_folders, image_per_class, True)\n",
    "train_size = int(image_per_class * num_classes * 0.7)\n",
    "test_size = int(image_per_class * num_classes * 0.2)\n",
    "valid_size = int(image_per_class * num_classes * 0.1)\n",
    "\n",
    "valid_dataset, valid_labels, test_dataset, test_labels, train_dataset, train_labels = merge_datasets(\n",
    "    train_datasets, train_size, test_size, valid_size)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "\n",
    "pickle_file = 'data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'valid_dataset': valid_dataset,\n",
    "        'valid_labels': valid_labels,\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "    }\n",
    "    Pickle.dump(save, f, Pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "\n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (2100, 32, 32) (2100,)\n",
      "Validation set (300, 32, 32) (300,)\n",
      "Training set (2100, 32, 32, 1) (2100, 3)\n",
      "Validation set (300, 32, 32, 1) (300, 3)\n",
      "Testing set (600, 32, 32, 1) (600, 3)\n",
      "Train on 2100 samples, validate on 300 samples\n",
      "Epoch 1/12\n",
      "2100/2100 [==============================] - 18s 9ms/sample - loss: 1.0948 - accuracy: 0.3719 - val_loss: 1.0834 - val_accuracy: 0.5400\n",
      "Epoch 2/12\n",
      "2100/2100 [==============================] - 11s 5ms/sample - loss: 1.0947 - accuracy: 0.3514 - val_loss: 1.0818 - val_accuracy: 0.5533\n",
      "Epoch 3/12\n",
      "2100/2100 [==============================] - 10s 5ms/sample - loss: 1.0913 - accuracy: 0.3695 - val_loss: 1.0801 - val_accuracy: 0.5633\n",
      "Epoch 4/12\n",
      "2100/2100 [==============================] - 12s 6ms/sample - loss: 1.0915 - accuracy: 0.3786 - val_loss: 1.0784 - val_accuracy: 0.5767\n",
      "Epoch 5/12\n",
      "2100/2100 [==============================] - 12s 6ms/sample - loss: 1.0899 - accuracy: 0.3714 - val_loss: 1.0766 - val_accuracy: 0.5867\n",
      "Epoch 6/12\n",
      "2100/2100 [==============================] - 11s 5ms/sample - loss: 1.0902 - accuracy: 0.3929 - val_loss: 1.0749 - val_accuracy: 0.5900\n",
      "Epoch 7/12\n",
      "2100/2100 [==============================] - 15s 7ms/sample - loss: 1.0860 - accuracy: 0.3900 - val_loss: 1.0732 - val_accuracy: 0.6067\n",
      "Epoch 8/12\n",
      "2100/2100 [==============================] - 11s 5ms/sample - loss: 1.0824 - accuracy: 0.4024 - val_loss: 1.0714 - val_accuracy: 0.6300\n",
      "Epoch 9/12\n",
      "2100/2100 [==============================] - 12s 6ms/sample - loss: 1.0805 - accuracy: 0.4257 - val_loss: 1.0696 - val_accuracy: 0.6633\n",
      "Epoch 10/12\n",
      "2100/2100 [==============================] - 14s 7ms/sample - loss: 1.0774 - accuracy: 0.4238 - val_loss: 1.0678 - val_accuracy: 0.6900\n",
      "Epoch 11/12\n",
      "2100/2100 [==============================] - 12s 6ms/sample - loss: 1.0779 - accuracy: 0.4162 - val_loss: 1.0660 - val_accuracy: 0.7367\n",
      "Epoch 12/12\n",
      "2100/2100 [==============================] - 9s 4ms/sample - loss: 1.0731 - accuracy: 0.4500 - val_loss: 1.0642 - val_accuracy: 0.7600\n",
      "600/600 [==============================] - 1s 1ms/sample - loss: 1.0648 - accuracy: 0.7617\n",
      "Test loss: 1.0648288424809773\n",
      "Test accuracy: 0.76166666\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from six.moves import cPickle as Pickle\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 3\n",
    "epochs = 12\n",
    "\n",
    "pickle_file = \"data.pickle\"\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = Pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    # print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, 32, 32, 1)).astype(np.float32)\n",
    "    labels = (np.arange(num_classes) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32, 32, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_dataset, train_labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(valid_dataset, valid_labels))\n",
    "score = model.evaluate(test_dataset, test_labels, verbose=1)\n",
    "model.save('model.h5')\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import csv\n",
    "import operator\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32, 1)\n",
      "[0 1 2]\n",
      "#########***#########\n",
      "Imagefile =  [[255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " ...\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]]\n",
      "Character =  0\n",
      "Confidence =  99.23480749130249 %\n",
      "Other predictions\n",
      "Character =  0\n",
      "Confidence =  99.23480749130249 %\n",
      "Character =  1\n",
      "Confidence =  0.703848572447896 %\n",
      "Character =  2\n",
      "Confidence =  0.06134964642114937 %\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"model.h5\")\n",
    "\n",
    "image = cv2.imread(\"D:\\\\Eng_dataset\\\\t1.png\", cv2.IMREAD_UNCHANGED)\n",
    "\"\"\"if image.shape[2] == 4:\n",
    "    image = read_transparent_png(image)\"\"\"\n",
    "image = clean(image)\n",
    "#cv2.imshow('gray', image)\n",
    "#cv2.waitKey(0)\n",
    "\n",
    "def predict(img):\n",
    "    image_data = img\n",
    "    dataset = np.asarray(image_data)\n",
    "    dataset = dataset.reshape((-1, 32, 32, 1)).astype(np.float32)\n",
    "    print(dataset.shape)\n",
    "    a = model.predict(dataset)[0]\n",
    "\n",
    "    classes = np.genfromtxt('classes.csv', delimiter=',')[:, 0].astype(int)\n",
    "\n",
    "    print(classes)\n",
    "    new = dict(zip(classes, a))\n",
    "    res = sorted(new.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    print(\"#########***#########\")\n",
    "    print(\"Imagefile = \", image)\n",
    "    print(\"Character = \", int(res[0][0]))\n",
    "    print(\"Confidence = \", res[0][1] * 100, \"%\")\n",
    "    if res[0][1] < 1:\n",
    "        print(\"Other predictions\")\n",
    "        for newtemp in res:\n",
    "            print(\"Character = \", newtemp[0])\n",
    "            print(\"Confidence = \", newtemp[1] * 100, \"%\")\n",
    "\n",
    "\n",
    "predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You\n"
     ]
    }
   ],
   "source": [
    "print(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "તમે\n"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "translator= Translator(from_lang=\"english\",to_lang=\"gujarati\")\n",
    "translation = translator.translate(words[0])\n",
    "print (translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
