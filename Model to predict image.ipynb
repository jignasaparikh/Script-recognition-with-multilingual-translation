{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_SIZE = 32\n",
    "\n",
    "\n",
    "def list_folders(root_folder):\n",
    "    \"\"\"Function to get subdir list\"\"\"\n",
    "    folder_list = []\n",
    "    for folder in sorted(os.listdir(root_folder)):\n",
    "        if os.path.isdir(os.path.join(root_folder, folder)):\n",
    "            folder_list.append(folder)\n",
    "    return folder_list\n",
    "\n",
    "\n",
    "def create_folders(root_folder, folder_list):\n",
    "    \"\"\"Function to create folders in new dataset\"\"\"\n",
    "    for folder in folder_list:\n",
    "        os.makedirs(os.path.join(root_folder, folder), exist_ok=True)\n",
    "\n",
    "\n",
    "def read_transparent_png(filename):\n",
    "    \"\"\"\n",
    "    Change transparent bg to white\n",
    "    \"\"\"\n",
    "    image_4channel = cv2.imread(filename, cv2.IMREAD_UNCHANGED)\n",
    "    alpha_channel = image_4channel[:, :, 0]\n",
    "    rgb_channels = image_4channel[:, :, :3]\n",
    "\n",
    "    # White Background Image\n",
    "    white_background_image = np.ones_like(rgb_channels, dtype=np.uint8) * 10\n",
    "\n",
    "    # Alpha factor\n",
    "    alpha_factor = alpha_channel[:, :, np.newaxis].astype(np.float32) / 255.0\n",
    "    alpha_factor = np.concatenate((alpha_factor, alpha_factor, alpha_factor), axis=2)\n",
    "\n",
    "    # Transparent Image Rendered on White Background\n",
    "    base = rgb_channels.astype(np.float32) * alpha_factor\n",
    "    white = white_background_image.astype(np.float32) * (1 - alpha_factor)\n",
    "    final_image = base + white\n",
    "    return final_image.astype(np.uint8)\n",
    "\n",
    "\n",
    "def clean(img):\n",
    "    \"\"\"Process an image\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    #(__, img_bw) = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "\n",
    "    #ctrs, __ = cv2.findContours(img_bw.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # take largest contour\n",
    "    #ctr = sorted(ctrs, key=lambda ctr: (cv2.boundingRect(ctr)[2] * cv2.boundingRect(ctr)[3]),\n",
    "                 #reverse=True)[0]\n",
    "    # Get bounding box\n",
    "    #x, y, w, h = cv2.boundingRect(ctr)\n",
    "\n",
    "    # Getting ROI\n",
    "    #roi = img_bw[y:y + h, x:x + w]\n",
    "    return crop(gray, IMAGE_SIZE)\n",
    "\n",
    "\n",
    "def crop(image, desired_size):\n",
    "    \"\"\"Crop and pad to req size\"\"\"\n",
    "    #old_size = image.shape[:2]  # old_size is in (height, width) format\n",
    "    #ratio = float(desired_size) / max(old_size)\n",
    "    #new_size = tuple([int(x * ratio) for x in old_size])\n",
    "\n",
    "    # new_size should be in (width, height) format\n",
    "    im = cv2.resize(image, (32, 32))\n",
    "\n",
    "    #delta_w = desired_size - new_size[1]\n",
    "    #delta_h = desired_size - new_size[0]\n",
    "    #top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    #left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "    #color = [0, 0, 0]\n",
    "    #new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "                                #value=color)\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def process_folder(folder):\n",
    "    \"\"\"Process all images in a folder\"\"\"\n",
    "    extension = '.png'\n",
    "    new_list = []\n",
    "    for img in sorted(os.listdir(folder)):\n",
    "        if img.endswith(extension):\n",
    "            image = read_transparent_png(os.path.join(folder, img))\n",
    "            new_img = clean(image)\n",
    "            new_list.append([img, new_img])\n",
    "            \"\"\"except:\n",
    "                print(\"\\t\" + img)\"\"\"\n",
    "    return new_list\n",
    "\n",
    "\n",
    "def save_new(folder, imglist):\n",
    "    \"\"\"Save newly created images\"\"\"\n",
    "    for img in imglist:\n",
    "        cv2.imwrite(os.path.join(folder, img[0]), img[1])\n",
    "\n",
    "\n",
    "def process_images(raw_folder, clean_folder, folder_list):\n",
    "    \"\"\"Process the images\"\"\"\n",
    "    for folder in folder_list:\n",
    "        print(folder)\n",
    "        imglist = process_folder(os.path.join(raw_folder, folder, 'output'))\n",
    "        save_new(os.path.join(clean_folder, folder), imglist)\n",
    "\n",
    "\n",
    "def skeletize(img):\n",
    "    size = np.size(img)\n",
    "    skel = np.zeros(img.shape, np.uint8)\n",
    "    element = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        eroded = cv2.erode(img, element)\n",
    "        temp = cv2.dilate(eroded, element)\n",
    "        temp = cv2.subtract(img, temp)\n",
    "        skel = cv2.bitwise_or(skel, temp)\n",
    "        img = eroded.copy()\n",
    "\n",
    "        zeroes = size - cv2.countNonZero(img)\n",
    "        if zeroes == size:\n",
    "            done = True\n",
    "\n",
    "    return skel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "करते\n",
      "50\n",
      "नहीं\n",
      "\n",
      "\n",
      "\n",
      "का\n",
      "और\n",
      "\n",
      "\n",
      "है।\n",
      "एक\n",
      "पर\n",
      "\n",
      "\n",
      "\n",
      "यह\n",
      "इस\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "भारत\n",
      "जाता\n",
      "\n",
      "\n",
      "नहीं\n",
      "या\n",
      "कर\n",
      "\n",
      "है।\n",
      "करने\n",
      "\n",
      "रूप\n",
      "संवत\n",
      "\n",
      "ईसा\n",
      "गया\n",
      "द्वारा\n",
      "साथ\n",
      "\n",
      "होता\n",
      "अपने\n",
      "बाद\n",
      "\n",
      "तथा\n",
      "तक\n",
      "इसके\n",
      "बाहरी\n",
      "लेख\n",
      "जा\n",
      "\n",
      "एवं\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = \"C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "words=[]\n",
    "a=['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40','41','42','43','44','45','46','47','48','49','50','51','52','53','54','55','56','57','58','59','60','61','62','63','64','65','66','67','68']\n",
    "for i in range(50):\n",
    "    hindi=pytesseract.image_to_string(Image.open(\"D:\\\\Dataset\\\\word_\"+a[i]+\"\\\\\"+a[i]+\".png\"),lang=\"hin\")\n",
    "    #print(hindi)\n",
    "    \n",
    "    words.append(hindi)\n",
    "\n",
    "img=cv2.imread(\"D:\\\\Dataset\\\\word_54\\\\54.png\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "hindi=pytesseract.image_to_string(Image.open(\"D:\\\\Dataset\\\\word_54\\\\54.png\"),lang=\"hin\")\n",
    "print(hindi)\n",
    "\n",
    "print(len(words))\n",
    "print(words[23])\n",
    "\n",
    "for i in range(50):\n",
    "    print(words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing word_60_original_60.png_b79e733d-6005-4aa9-8f96-cba462c23a6e.png:  10%| | 19/200 [00:00<00:01, 117.69 Sample"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 13082 image(s) found.\n",
      "Output directory set to D:\\Dataset\\clean\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing word_39_original_39.png_06111604-69f9-4748-abac-e3ebfda70412.png: 100%|█| 200/200 [00:01<00:00, 132.94 Sampl\n",
      "Executing Pipeline:   0%|                                                                | 0/200 [00:00<?, ? Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 2 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_1\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1.png: 100%|████████████████████████████████████████████████████████| 200/200 [00:23<00:00,  8.40 Samples/s]\n",
      "Processing 10.png:   2%|█▏                                                       | 4/200 [00:00<00:07, 26.05 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_10\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 10.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:04<00:00, 49.74 Samples/s]\n",
      "Processing 11.png:   1%|▌                                                        | 2/200 [00:00<00:12, 15.44 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_11\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 11.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:04<00:00, 42.37 Samples/s]\n",
      "Processing 12.png:   5%|██▊                                                     | 10/200 [00:00<00:03, 62.67 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_12\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 12.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 67.87 Samples/s]\n",
      "Processing 13.png:   6%|███                                                     | 11/200 [00:00<00:02, 69.57 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_13\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 13.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 74.24 Samples/s]\n",
      "Processing 14.png:   4%|██▌                                                      | 9/200 [00:00<00:03, 54.98 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_14\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 14.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 61.55 Samples/s]\n",
      "Processing 15.png:   3%|█▋                                                       | 6/200 [00:00<00:05, 33.81 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_15\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 15.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 59.68 Samples/s]\n",
      "Processing 16.png:   6%|███                                                     | 11/200 [00:00<00:02, 72.27 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_16\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 16.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 71.49 Samples/s]\n",
      "Processing 17.png:   6%|███                                                     | 11/200 [00:00<00:03, 58.46 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_17\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 17.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 84.26 Samples/s]\n",
      "Processing 18.png:   6%|███                                                     | 11/200 [00:00<00:03, 56.65 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_18\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 18.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 91.32 Samples/s]\n",
      "Processing 19.png:   6%|███                                                     | 11/200 [00:00<00:03, 62.67 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_19\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 19.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 68.04 Samples/s]\n",
      "Processing 2.png:   2%|█▏                                                        | 4/200 [00:00<00:24,  8.03 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 2 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_2\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2.png: 100%|████████████████████████████████████████████████████████| 200/200 [00:10<00:00, 18.30 Samples/s]\n",
      "Processing 20.png:   4%|██▎                                                      | 8/200 [00:00<00:04, 46.20 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_20\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 20.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 50.10 Samples/s]\n",
      "Processing 21.png:   4%|██▌                                                      | 9/200 [00:00<00:03, 53.44 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_21\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 21.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 50.99 Samples/s]\n",
      "Processing 22.png:   5%|██▊                                                     | 10/200 [00:00<00:03, 49.55 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_22\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 22.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 69.17 Samples/s]\n",
      "Processing 23.png:   5%|██▊                                                     | 10/200 [00:00<00:03, 55.16 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_23\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 23.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 67.28 Samples/s]\n",
      "Processing 24.png:   4%|██▌                                                      | 9/200 [00:00<00:03, 54.26 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_24\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 24.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 58.72 Samples/s]\n",
      "Processing 25.png:   4%|██▎                                                      | 8/200 [00:00<00:04, 47.28 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_25\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 25.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 70.12 Samples/s]\n",
      "Processing 26.png:   4%|██▌                                                      | 9/200 [00:00<00:03, 51.86 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_26\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 26.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 65.67 Samples/s]\n",
      "Processing 27.png:   7%|███▉                                                    | 14/200 [00:00<00:02, 78.21 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_27\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 27.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 77.80 Samples/s]\n",
      "Processing 28.png:   4%|█▉                                                       | 7/200 [00:00<00:04, 46.00 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_28\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 28.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 66.80 Samples/s]\n",
      "Processing 29.png:   4%|██▌                                                      | 9/200 [00:00<00:03, 51.81 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_29\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 29.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 52.40 Samples/s]\n",
      "Executing Pipeline:   0%|                                                                | 0/200 [00:00<?, ? Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 2 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_3\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 3.png: 100%|████████████████████████████████████████████████████████| 200/200 [00:16<00:00, 12.01 Samples/s]\n",
      "Processing 30.png:   6%|███▎                                                    | 12/200 [00:00<00:02, 74.48 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_30\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 30.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 72.15 Samples/s]\n",
      "Processing 31.png:   4%|██▌                                                      | 9/200 [00:00<00:03, 48.25 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_31\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 31.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 64.10 Samples/s]\n",
      "Processing 32.png:   4%|██▎                                                      | 8/200 [00:00<00:04, 39.49 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_32\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 32.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 50.95 Samples/s]\n",
      "Processing 33.png:   8%|████▍                                                   | 16/200 [00:00<00:01, 93.73 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_33\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 33.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 88.96 Samples/s]\n",
      "Processing 34.png:   2%|█▍                                                       | 5/200 [00:00<00:07, 26.86 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_34\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 34.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 55.06 Samples/s]\n",
      "Processing 35.png:   4%|██▌                                                      | 9/200 [00:00<00:04, 44.46 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_35\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 35.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 59.10 Samples/s]\n",
      "Processing 36.png:   4%|██▎                                                      | 8/200 [00:00<00:03, 49.31 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_36\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 36.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:04<00:00, 45.00 Samples/s]\n",
      "Processing 37.png:   4%|█▉                                                       | 7/200 [00:00<00:04, 45.30 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_37\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 37.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 58.97 Samples/s]\n",
      "Processing 38.png:   4%|██▎                                                      | 8/200 [00:00<00:03, 53.13 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_38\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 38.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 53.24 Samples/s]\n",
      "Processing 39.png:   4%|██▌                                                      | 9/200 [00:00<00:03, 55.55 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_39\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 39.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 50.93 Samples/s]\n",
      "Processing 444.PNG:   0%|▎                                                       | 1/200 [00:00<00:26,  7.46 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 2 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_4\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 4.png: 100%|████████████████████████████████████████████████████████| 200/200 [00:15<00:00, 13.07 Samples/s]\n",
      "Processing 40.png:   4%|██▌                                                      | 9/200 [00:00<00:03, 55.14 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_40\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 40.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 51.97 Samples/s]\n",
      "Processing 41.png:   5%|██▊                                                     | 10/200 [00:00<00:02, 64.73 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_41\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 41.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 61.70 Samples/s]\n",
      "Processing 42.png:   6%|███▎                                                    | 12/200 [00:00<00:02, 73.09 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_42\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 42.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 71.84 Samples/s]\n",
      "Processing 43.png:   6%|███                                                     | 11/200 [00:00<00:03, 62.76 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_43\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 43.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 59.54 Samples/s]\n",
      "Processing 44.png:   4%|██▌                                                      | 9/200 [00:00<00:03, 59.93 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_44\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 44.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 61.38 Samples/s]\n",
      "Processing 45.png:   2%|█▍                                                       | 5/200 [00:00<00:05, 35.93 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_45\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 45.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:04<00:00, 46.47 Samples/s]\n",
      "Processing 46.png:   4%|██▎                                                      | 8/200 [00:00<00:03, 48.13 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_46\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 46.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:04<00:00, 47.31 Samples/s]\n",
      "Processing 47.png:   4%|██▌                                                      | 9/200 [00:00<00:04, 47.34 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_47\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 47.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 57.15 Samples/s]\n",
      "Processing 48.png:   5%|██▊                                                     | 10/200 [00:00<00:03, 55.68 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_48\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 48.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 70.85 Samples/s]\n",
      "Processing 49.png:   4%|██▎                                                      | 8/200 [00:00<00:03, 51.93 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_49\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 49.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 69.67 Samples/s]\n",
      "Processing 5.png:   2%|▊                                                         | 3/200 [00:00<00:07, 26.97 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 2 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_5\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5.png: 100%|████████████████████████████████████████████████████████| 200/200 [00:18<00:00, 10.90 Samples/s]\n",
      "Processing 50.png:   3%|█▋                                                       | 6/200 [00:00<00:04, 41.78 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_50\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 50.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 51.67 Samples/s]\n",
      "Processing 51.png:   3%|█▋                                                       | 6/200 [00:00<00:04, 43.41 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_51\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 51.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:04<00:00, 45.11 Samples/s]\n",
      "Processing 52.png:   3%|█▋                                                       | 6/200 [00:00<00:04, 39.72 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_52\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 52.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:04<00:00, 47.47 Samples/s]\n",
      "Processing 53.png:   4%|██▎                                                      | 8/200 [00:00<00:03, 48.26 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_53\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 53.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 51.11 Samples/s]\n",
      "Processing 54.png:   3%|█▋                                                       | 6/200 [00:00<00:04, 40.06 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_54\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 54.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:04<00:00, 44.83 Samples/s]\n",
      "Processing 55.png:   5%|██▊                                                     | 10/200 [00:00<00:03, 55.19 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_55\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 55.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:02<00:00, 87.04 Samples/s]\n",
      "Processing 56.png:   5%|██▊                                                     | 10/200 [00:00<00:02, 64.39 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_56\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 56.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 64.69 Samples/s]\n",
      "Processing 57.png:   6%|███                                                     | 11/200 [00:00<00:03, 62.71 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_57\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 57.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 62.61 Samples/s]\n",
      "Processing 58.png:   2%|█▍                                                       | 5/200 [00:00<00:05, 36.80 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_58\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 58.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:04<00:00, 46.59 Samples/s]\n",
      "Processing 59.png:   5%|██▊                                                     | 10/200 [00:00<00:02, 64.99 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_59\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 59.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 59.23 Samples/s]\n",
      "Processing 6.png:   6%|███▏                                                     | 11/200 [00:00<00:02, 63.23 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_6\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 6.png: 100%|████████████████████████████████████████████████████████| 200/200 [00:02<00:00, 67.45 Samples/s]\n",
      "Processing 60.png:   5%|██▊                                                     | 10/200 [00:00<00:03, 56.09 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_60\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 60.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 55.74 Samples/s]\n",
      "Processing 61.png:   4%|██▌                                                      | 9/200 [00:00<00:03, 55.19 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_61\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 61.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 52.86 Samples/s]\n",
      "Processing 62.png:   3%|█▋                                                       | 6/200 [00:00<00:05, 38.45 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_62\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 62.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:04<00:00, 42.45 Samples/s]\n",
      "Processing 63.png:   3%|█▋                                                       | 6/200 [00:00<00:05, 35.01 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_63\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 63.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:05<00:00, 33.55 Samples/s]\n",
      "Processing 64.png:   4%|█▉                                                       | 7/200 [00:00<00:04, 45.99 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_64\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 64.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 55.74 Samples/s]\n",
      "Processing 65.png:   4%|██▌                                                      | 9/200 [00:00<00:03, 52.39 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_65\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 65.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 57.14 Samples/s]\n",
      "Processing 66.png:   2%|█▍                                                       | 5/200 [00:00<00:06, 28.71 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_66\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 66.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:04<00:00, 42.31 Samples/s]\n",
      "Processing 67.png:   2%|█▍                                                       | 5/200 [00:00<00:05, 33.98 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_67\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 67.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:04<00:00, 48.23 Samples/s]\n",
      "Processing 68.png:   4%|█▉                                                       | 7/200 [00:00<00:04, 44.76 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_68\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 68.png: 100%|███████████████████████████████████████████████████████| 200/200 [00:03<00:00, 52.04 Samples/s]\n",
      "Processing 7.png:   5%|██▊                                                      | 10/200 [00:00<00:03, 63.04 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_7\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 7.png: 100%|████████████████████████████████████████████████████████| 200/200 [00:03<00:00, 60.18 Samples/s]\n",
      "Processing 8.png:   4%|██                                                        | 7/200 [00:00<00:04, 45.58 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_8\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 8.png: 100%|████████████████████████████████████████████████████████| 200/200 [00:03<00:00, 58.67 Samples/s]\n",
      "Processing 9.png:   4%|██▌                                                       | 9/200 [00:00<00:03, 56.22 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to D:\\Dataset\\word_9\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 9.png: 100%|████████████████████████████████████████████████████████| 200/200 [00:03<00:00, 57.61 Samples/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import Augmentor\n",
    "\n",
    "\n",
    "folder = \"D:\\\\Dataset\"\n",
    "for f in list_folders(folder):\n",
    "    if os.path.isdir(os.path.join(folder, f, 'output')):\n",
    "        shutil.rmtree(os.path.join(folder, f, 'output'))\n",
    "    p = Augmentor.Pipeline(os.path.join(folder, f))\n",
    "    p.random_distortion(probability=1, grid_width=10, grid_height=10, magnitude=8)\n",
    "    p.sample(200, multi_threaded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word_1', 'word_10', 'word_11', 'word_12', 'word_13', 'word_14', 'word_15', 'word_16', 'word_17', 'word_18', 'word_19', 'word_2', 'word_20', 'word_21', 'word_22', 'word_23', 'word_24', 'word_25', 'word_26', 'word_27', 'word_28', 'word_29', 'word_3', 'word_30', 'word_31', 'word_32', 'word_33', 'word_34', 'word_35', 'word_36', 'word_37', 'word_38', 'word_39', 'word_4', 'word_40', 'word_41', 'word_42', 'word_43', 'word_44', 'word_45', 'word_46', 'word_47', 'word_48', 'word_49', 'word_5', 'word_50', 'word_51', 'word_52', 'word_53', 'word_54', 'word_55', 'word_56', 'word_57', 'word_58', 'word_59', 'word_6', 'word_60', 'word_61', 'word_62', 'word_63', 'word_64', 'word_65', 'word_66', 'word_67', 'word_68', 'word_7', 'word_8', 'word_9']\n",
      "word_1\n",
      "word_10\n",
      "word_11\n",
      "word_12\n",
      "word_13\n",
      "word_14\n",
      "word_15\n",
      "word_16\n",
      "word_17\n",
      "word_18\n",
      "word_19\n",
      "word_2\n",
      "word_20\n",
      "word_21\n",
      "word_22\n",
      "word_23\n",
      "word_24\n",
      "word_25\n",
      "word_26\n",
      "word_27\n",
      "word_28\n",
      "word_29\n",
      "word_3\n",
      "word_30\n",
      "word_31\n",
      "word_32\n",
      "word_33\n",
      "word_34\n",
      "word_35\n",
      "word_36\n",
      "word_37\n",
      "word_38\n",
      "word_39\n",
      "word_4\n",
      "word_40\n",
      "word_41\n",
      "word_42\n",
      "word_43\n",
      "word_44\n",
      "word_45\n",
      "word_46\n",
      "word_47\n",
      "word_48\n",
      "word_49\n",
      "word_5\n",
      "word_50\n",
      "word_51\n",
      "word_52\n",
      "word_53\n",
      "word_54\n",
      "word_55\n",
      "word_56\n",
      "word_57\n",
      "word_58\n",
      "word_59\n",
      "word_6\n",
      "word_60\n",
      "word_61\n",
      "word_62\n",
      "word_63\n",
      "word_64\n",
      "word_65\n",
      "word_66\n",
      "word_67\n",
      "word_68\n",
      "word_7\n",
      "word_8\n",
      "word_9\n"
     ]
    }
   ],
   "source": [
    "RAW_FOLDER = \"D:\\\\Dataset\"\n",
    "CLEAN_FOLDER = \"D:\\\\Dataset\\\\clean\"\n",
    "\n",
    "FOLDER_LIST = list_folders(RAW_FOLDER)\n",
    "print(FOLDER_LIST)\n",
    "\n",
    "create_folders(CLEAN_FOLDER, FOLDER_LIST)\n",
    "process_images(RAW_FOLDER, CLEAN_FOLDER, FOLDER_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Dataset\\clean\\word_1\n",
      "Full dataset tensor: (97, 32, 32)\n",
      "Mean: 0.9396444\n",
      "Standard deviation: 0.23814449\n",
      "D:\\Dataset\\clean\\word_10\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.944624\n",
      "Standard deviation: 0.22871265\n",
      "D:\\Dataset\\clean\\word_11\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.94178224\n",
      "Standard deviation: 0.2341548\n",
      "D:\\Dataset\\clean\\word_12\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.92589843\n",
      "Standard deviation: 0.2619361\n",
      "D:\\Dataset\\clean\\word_13\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9470605\n",
      "Standard deviation: 0.22391264\n",
      "D:\\Dataset\\clean\\word_14\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9335742\n",
      "Standard deviation: 0.2490249\n",
      "D:\\Dataset\\clean\\word_15\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9311279\n",
      "Standard deviation: 0.25323647\n",
      "D:\\Dataset\\clean\\word_16\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.94819826\n",
      "Standard deviation: 0.22162656\n",
      "D:\\Dataset\\clean\\word_17\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9743262\n",
      "Standard deviation: 0.15816033\n",
      "D:\\Dataset\\clean\\word_18\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9909619\n",
      "Standard deviation: 0.09463825\n",
      "D:\\Dataset\\clean\\word_19\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9358447\n",
      "Standard deviation: 0.24502935\n",
      "D:\\Dataset\\clean\\word_2\n",
      "Full dataset tensor: (121, 32, 32)\n",
      "Mean: 0.93823445\n",
      "Standard deviation: 0.24072926\n",
      "D:\\Dataset\\clean\\word_20\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9210693\n",
      "Standard deviation: 0.26963052\n",
      "D:\\Dataset\\clean\\word_21\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9269726\n",
      "Standard deviation: 0.26018134\n",
      "D:\\Dataset\\clean\\word_22\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.93375\n",
      "Standard deviation: 0.24871859\n",
      "D:\\Dataset\\clean\\word_23\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.94490236\n",
      "Standard deviation: 0.2281708\n",
      "D:\\Dataset\\clean\\word_24\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.91962403\n",
      "Standard deviation: 0.27187437\n",
      "D:\\Dataset\\clean\\word_25\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.94333005\n",
      "Standard deviation: 0.23121081\n",
      "D:\\Dataset\\clean\\word_26\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.935542\n",
      "Standard deviation: 0.24556705\n",
      "D:\\Dataset\\clean\\word_27\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.93885744\n",
      "Standard deviation: 0.23959166\n",
      "D:\\Dataset\\clean\\word_28\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9377197\n",
      "Standard deviation: 0.24166389\n",
      "D:\\Dataset\\clean\\word_29\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9181006\n",
      "Standard deviation: 0.2742114\n",
      "D:\\Dataset\\clean\\word_3\n",
      "Full dataset tensor: (93, 32, 32)\n",
      "Mean: 0.9277239\n",
      "Standard deviation: 0.25894454\n",
      "D:\\Dataset\\clean\\word_30\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9375586\n",
      "Standard deviation: 0.24195552\n",
      "D:\\Dataset\\clean\\word_31\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.94029784\n",
      "Standard deviation: 0.2369342\n",
      "D:\\Dataset\\clean\\word_32\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.92105466\n",
      "Standard deviation: 0.26965338\n",
      "D:\\Dataset\\clean\\word_33\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9814404\n",
      "Standard deviation: 0.1349634\n",
      "D:\\Dataset\\clean\\word_34\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.92098635\n",
      "Standard deviation: 0.26976013\n",
      "D:\\Dataset\\clean\\word_35\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9288916\n",
      "Standard deviation: 0.2570058\n",
      "D:\\Dataset\\clean\\word_36\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.92019534\n",
      "Standard deviation: 0.2709906\n",
      "D:\\Dataset\\clean\\word_37\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9245166\n",
      "Standard deviation: 0.26416972\n",
      "D:\\Dataset\\clean\\word_38\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9231055\n",
      "Standard deviation: 0.26642403\n",
      "D:\\Dataset\\clean\\word_39\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.92355466\n",
      "Standard deviation: 0.2657093\n",
      "D:\\Dataset\\clean\\word_4\n",
      "Full dataset tensor: (94, 32, 32)\n",
      "Mean: 0.93801945\n",
      "Standard deviation: 0.24112022\n",
      "D:\\Dataset\\clean\\word_40\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9237354\n",
      "Standard deviation: 0.2654211\n",
      "D:\\Dataset\\clean\\word_41\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.92961913\n",
      "Standard deviation: 0.25578776\n",
      "D:\\Dataset\\clean\\word_42\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.93938476\n",
      "Standard deviation: 0.23862319\n",
      "D:\\Dataset\\clean\\word_43\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.92645997\n",
      "Standard deviation: 0.26102087\n",
      "D:\\Dataset\\clean\\word_44\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9310254\n",
      "Standard deviation: 0.25341097\n",
      "D:\\Dataset\\clean\\word_45\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9149805\n",
      "Standard deviation: 0.27891076\n",
      "D:\\Dataset\\clean\\word_46\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9136279\n",
      "Standard deviation: 0.28091267\n",
      "D:\\Dataset\\clean\\word_47\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9174072\n",
      "Standard deviation: 0.2752657\n",
      "D:\\Dataset\\clean\\word_48\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9412744\n",
      "Standard deviation: 0.23511039\n",
      "D:\\Dataset\\clean\\word_49\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9453955\n",
      "Standard deviation: 0.22720663\n",
      "D:\\Dataset\\clean\\word_5\n",
      "Full dataset tensor: (106, 32, 32)\n",
      "Mean: 0.92476785\n",
      "Standard deviation: 0.26376557\n",
      "D:\\Dataset\\clean\\word_50\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9362842\n",
      "Standard deviation: 0.24424604\n",
      "D:\\Dataset\\clean\\word_51\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9164014\n",
      "Standard deviation: 0.27678493\n",
      "D:\\Dataset\\clean\\word_52\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.924209\n",
      "Standard deviation: 0.26466343\n",
      "D:\\Dataset\\clean\\word_53\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9343701\n",
      "Standard deviation: 0.247634\n",
      "D:\\Dataset\\clean\\word_54\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.91955566\n",
      "Standard deviation: 0.27197984\n",
      "D:\\Dataset\\clean\\word_55\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9934033\n",
      "Standard deviation: 0.080951616\n",
      "D:\\Dataset\\clean\\word_56\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9423926\n",
      "Standard deviation: 0.2329996\n",
      "D:\\Dataset\\clean\\word_57\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.94720215\n",
      "Standard deviation: 0.22362967\n",
      "D:\\Dataset\\clean\\word_58\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.91765136\n",
      "Standard deviation: 0.27489513\n",
      "D:\\Dataset\\clean\\word_59\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9414795\n",
      "Standard deviation: 0.23472507\n",
      "D:\\Dataset\\clean\\word_6\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.94092774\n",
      "Standard deviation: 0.2357599\n",
      "D:\\Dataset\\clean\\word_60\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.92801267\n",
      "Standard deviation: 0.2584669\n",
      "D:\\Dataset\\clean\\word_61\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9213965\n",
      "Standard deviation: 0.26911893\n",
      "D:\\Dataset\\clean\\word_62\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9149609\n",
      "Standard deviation: 0.2789398\n",
      "D:\\Dataset\\clean\\word_63\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.91743654\n",
      "Standard deviation: 0.2752213\n",
      "D:\\Dataset\\clean\\word_64\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.93024904\n",
      "Standard deviation: 0.2547269\n",
      "D:\\Dataset\\clean\\word_65\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9220166\n",
      "Standard deviation: 0.26814547\n",
      "D:\\Dataset\\clean\\word_66\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.91584474\n",
      "Standard deviation: 0.27762055\n",
      "D:\\Dataset\\clean\\word_67\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.92067385\n",
      "Standard deviation: 0.27024716\n",
      "D:\\Dataset\\clean\\word_68\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.91771483\n",
      "Standard deviation: 0.27479866\n",
      "D:\\Dataset\\clean\\word_7\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.93178713\n",
      "Standard deviation: 0.25211087\n",
      "D:\\Dataset\\clean\\word_8\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.9393604\n",
      "Standard deviation: 0.23866814\n",
      "D:\\Dataset\\clean\\word_9\n",
      "Full dataset tensor: (200, 32, 32)\n",
      "Mean: 0.93344235\n",
      "Standard deviation: 0.24925427\n",
      "68\n",
      "9 18 63\n",
      "0 9\n",
      "9 27\n",
      "27 90\n",
      "Training set (4331, 32, 32) (4331,)\n",
      "Test set (1237, 32, 32) (1237,)\n",
      "Validation set (618, 32, 32) (618,)\n",
      "Compressed pickle size: 25363093\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from six.moves import cPickle as Pickle\n",
    "import csv\n",
    "\n",
    "DATA_FOLDER = \"D:\\\\Dataset\\\\clean\"\n",
    "image_size = 32\n",
    "pixel_depth = 255\n",
    "pickle_extension = '.pickle'\n",
    "num_classes = 68\n",
    "image_per_class = 91\n",
    "\n",
    "\n",
    "def get_folders(path):\n",
    "    data_folders = [os.path.join(path, d) for d in sorted(os.listdir(path))\n",
    "                    if os.path.isdir(os.path.join(path, d))]\n",
    "\n",
    "    if len(data_folders) != num_classes:\n",
    "        raise Exception(\n",
    "            'Expected %d folders, one per class. Found %d instead.' % (\n",
    "                num_classes, len(data_folders)))\n",
    "\n",
    "    return data_folders\n",
    "\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    \"\"\"Load the data for a single letter label.\"\"\"\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "    print(folder)\n",
    "    image_index = -1\n",
    "    for image_index, image in enumerate(image_files):\n",
    "        image_file = os.path.join(folder, image)\n",
    "        try:\n",
    "            image_data = 1 * (cv2.imread(image_file, cv2.IMREAD_UNCHANGED).astype(float) > pixel_depth / 2)\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            dataset[image_index, :, :] = image_data\n",
    "        except IOError as err:\n",
    "            print('Could not read:', image_file, ':', err, '- it\\'s ok, skipping.')\n",
    "\n",
    "    num_images = image_index + 1\n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' % (num_images, min_num_images))\n",
    "\n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Standard deviation:', np.std(dataset))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + pickle_extension\n",
    "        dataset_names.append(folder)\n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            # You may override by setting force=True.\n",
    "            print('%s already present - Skipping pickling.' % set_filename)\n",
    "        else:\n",
    "            # print('Pickling %s.' % set_filename)\n",
    "            dataset = load_letter(folder, min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    Pickle.dump(dataset, f, Pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':', e)\n",
    "\n",
    "    return dataset_names\n",
    "\n",
    "\n",
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "        labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, test_size=0, valid_size=0):\n",
    "    num_classes = len(pickle_files)\n",
    "    print(num_classes)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "    test_dataset, test_labels = make_arrays(test_size, image_size)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "    valid_size_per_class = valid_size // num_classes\n",
    "    test_size_per_class = test_size // num_classes\n",
    "    train_size_per_class = train_size // num_classes\n",
    "\n",
    "    print(valid_size_per_class, test_size_per_class, train_size_per_class)\n",
    "\n",
    "    start_valid, start_test, start_train = 0, valid_size_per_class, (valid_size_per_class + test_size_per_class)\n",
    "    end_valid = valid_size_per_class\n",
    "    end_test = end_valid + test_size_per_class\n",
    "    end_train = end_test + train_size_per_class\n",
    "\n",
    "    print(start_valid, end_valid)\n",
    "    print(start_test, end_test)\n",
    "    print(start_train,end_train)\n",
    "\n",
    "    s_valid, s_test, s_train = 0, 0, 0\n",
    "    e_valid, e_test, e_train = valid_size_per_class, test_size_per_class, train_size_per_class\n",
    "    temp = []\n",
    "    for label, pickle_file in enumerate(pickle_files):\n",
    "        temp.append([label, pickle_file[-4:]])\n",
    "        try:\n",
    "            with open(pickle_file + pickle_extension, 'rb') as f:\n",
    "                letter_set = Pickle.load(f)\n",
    "                # let's shuffle the letters to have random validation and training set\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter = letter_set[:end_valid, :, :]\n",
    "                    valid_dataset[s_valid:e_valid, :, :] = valid_letter\n",
    "                    valid_labels[s_valid:e_valid] = label\n",
    "                    s_valid += valid_size_per_class\n",
    "                    e_valid += valid_size_per_class\n",
    "\n",
    "                if test_dataset is not None:\n",
    "                    test_letter = letter_set[start_test:end_test, :, :]\n",
    "                    test_dataset[s_test:e_test, :, :] = test_letter\n",
    "                    test_labels[s_test:e_test] = label\n",
    "                    s_test += test_size_per_class\n",
    "                    e_test += test_size_per_class\n",
    "\n",
    "                train_letter = letter_set[start_train:end_train, :, :]\n",
    "                train_dataset[s_train:e_train, :, :] = train_letter\n",
    "                train_labels[s_train:e_train] = label\n",
    "                s_train += train_size_per_class\n",
    "                e_train += train_size_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':', e)\n",
    "            raise\n",
    "    with open('classes.csv', 'w') as my_csv:\n",
    "        writer = csv.writer(my_csv, delimiter=',')\n",
    "        writer.writerows(temp)\n",
    "    return valid_dataset, valid_labels, test_dataset, test_labels, train_dataset, train_labels\n",
    "\n",
    "\n",
    "data_folders = get_folders(DATA_FOLDER)\n",
    "train_datasets = maybe_pickle(data_folders, image_per_class, True)\n",
    "train_size = int(image_per_class * num_classes * 0.7)\n",
    "test_size = int(image_per_class * num_classes * 0.2)\n",
    "valid_size = int(image_per_class * num_classes * 0.1)\n",
    "\n",
    "valid_dataset, valid_labels, test_dataset, test_labels, train_dataset, train_labels = merge_datasets(\n",
    "    train_datasets, train_size, test_size, valid_size)\n",
    "word[2] = \"एवं\"\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "\n",
    "pickle_file = 'data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'valid_dataset': valid_dataset,\n",
    "        'valid_labels': valid_labels,\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "    }\n",
    "    Pickle.dump(save, f, Pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "\n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (2866, 60, 60) (2866,)\n",
      "Validation set (409, 60, 60) (409,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 10317600 into shape (32,32,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-31445d68d870>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-31445d68d870>\u001b[0m in \u001b[0;36mreformat\u001b[1;34m(dataset, labels)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 10317600 into shape (32,32,1)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from six.moves import cPickle as Pickle\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 68\n",
    "epochs = 12\n",
    "\n",
    "pickle_file = \"data.pickle\"\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = Pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    # print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, 32, 32, 1)).astype(np.float32)\n",
    "    labels = (np.arange(num_classes) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32, 32, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_dataset, train_labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(valid_dataset, valid_labels))\n",
    "score = model.evaluate(test_dataset, test_labels, verbose=1)\n",
    "model.save('model.h5')\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import csv\n",
    "import operator\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32, 1)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67]\n",
      "#########***#########\n",
      "Imagefile =  [[255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " ...\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]]\n",
      "Character =  2\n",
      "Confidence =  89.531010389328 %\n",
      "Other predictions\n",
      "Character =  2\n",
      "Confidence =  89.531010389328 %\n",
      "Character =  33\n",
      "Confidence =  9.733594954013824 %\n",
      "Character =  67\n",
      "Confidence =  0.3957119770348072 %\n",
      "Character =  5\n",
      "Confidence =  0.3152725286781788 %\n",
      "Character =  64\n",
      "Confidence =  0.024404030409641564 %\n",
      "Character =  24\n",
      "Confidence =  1.5333534975070506e-05 %\n",
      "Character =  35\n",
      "Confidence =  4.107442563849872e-06 %\n",
      "Character =  30\n",
      "Confidence =  6.220513881771694e-07 %\n",
      "Character =  41\n",
      "Confidence =  1.5539874942405163e-08 %\n",
      "Character =  0\n",
      "Confidence =  1.1729084370415421e-08 %\n",
      "Character =  45\n",
      "Confidence =  9.17191184113264e-09 %\n",
      "Character =  59\n",
      "Confidence =  3.79785924931042e-09 %\n",
      "Character =  23\n",
      "Confidence =  1.1320091565514279e-09 %\n",
      "Character =  37\n",
      "Confidence =  1.0883082629531415e-09 %\n",
      "Character =  19\n",
      "Confidence =  5.534802759339652e-11 %\n",
      "Character =  40\n",
      "Confidence =  3.5569312252514695e-11 %\n",
      "Character =  9\n",
      "Confidence =  3.333684849980874e-12 %\n",
      "Character =  66\n",
      "Confidence =  2.200335962423191e-12 %\n",
      "Character =  63\n",
      "Confidence =  1.9164482538213496e-13 %\n",
      "Character =  6\n",
      "Confidence =  1.621239589890452e-14 %\n",
      "Character =  13\n",
      "Confidence =  7.19036607454621e-15 %\n",
      "Character =  49\n",
      "Confidence =  2.5790571674562244e-15 %\n",
      "Character =  51\n",
      "Confidence =  1.5262756159746692e-15 %\n",
      "Character =  56\n",
      "Confidence =  1.9948566825785552e-16 %\n",
      "Character =  62\n",
      "Confidence =  8.571593957107511e-17 %\n",
      "Character =  16\n",
      "Confidence =  5.99097139170106e-17 %\n",
      "Character =  11\n",
      "Confidence =  5.1646582848154477e-17 %\n",
      "Character =  12\n",
      "Confidence =  3.3635161864573304e-17 %\n",
      "Character =  8\n",
      "Confidence =  2.80853182679755e-18 %\n",
      "Character =  65\n",
      "Confidence =  2.5848348857406486e-18 %\n",
      "Character =  44\n",
      "Confidence =  1.78954111644926e-18 %\n",
      "Character =  10\n",
      "Confidence =  1.1511559160996511e-18 %\n",
      "Character =  27\n",
      "Confidence =  8.684873789368976e-19 %\n",
      "Character =  61\n",
      "Confidence =  6.506828880540861e-19 %\n",
      "Character =  58\n",
      "Confidence =  4.554783609111584e-19 %\n",
      "Character =  32\n",
      "Confidence =  1.2714209291659897e-19 %\n",
      "Character =  60\n",
      "Confidence =  8.295383095394543e-20 %\n",
      "Character =  53\n",
      "Confidence =  3.870417668020397e-20 %\n",
      "Character =  17\n",
      "Confidence =  2.298804543992915e-20 %\n",
      "Character =  55\n",
      "Confidence =  1.6197943499812293e-20 %\n",
      "Character =  14\n",
      "Confidence =  1.1081743816056901e-20 %\n",
      "Character =  34\n",
      "Confidence =  2.7039343486153698e-21 %\n",
      "Character =  18\n",
      "Confidence =  1.2633504845196825e-21 %\n",
      "Character =  7\n",
      "Confidence =  1.5960555295250388e-22 %\n",
      "Character =  28\n",
      "Confidence =  7.169931129574357e-23 %\n",
      "Character =  22\n",
      "Confidence =  6.3881678770885544e-24 %\n",
      "Character =  15\n",
      "Confidence =  1.5757151455143677e-24 %\n",
      "Character =  57\n",
      "Confidence =  4.21085053410633e-25 %\n",
      "Character =  21\n",
      "Confidence =  1.1768288036066291e-25 %\n",
      "Character =  46\n",
      "Confidence =  9.788649683319928e-26 %\n",
      "Character =  43\n",
      "Confidence =  1.933818312698203e-26 %\n",
      "Character =  54\n",
      "Confidence =  9.049252677601857e-27 %\n",
      "Character =  47\n",
      "Confidence =  6.25094568151458e-27 %\n",
      "Character =  20\n",
      "Confidence =  1.871783477243727e-27 %\n",
      "Character =  39\n",
      "Confidence =  1.0902320780147724e-28 %\n",
      "Character =  38\n",
      "Confidence =  5.734609223678602e-29 %\n",
      "Character =  50\n",
      "Confidence =  1.5217825263344147e-29 %\n",
      "Character =  1\n",
      "Confidence =  1.1625260620451459e-29 %\n",
      "Character =  26\n",
      "Confidence =  7.562796812794964e-30 %\n",
      "Character =  36\n",
      "Confidence =  9.20654742808899e-31 %\n",
      "Character =  4\n",
      "Confidence =  4.826515519629002e-31 %\n",
      "Character =  42\n",
      "Confidence =  3.173673116746938e-31 %\n",
      "Character =  31\n",
      "Confidence =  1.3633352256768197e-31 %\n",
      "Character =  25\n",
      "Confidence =  1.2010796052575337e-32 %\n",
      "Character =  48\n",
      "Confidence =  1.629276675579165e-34 %\n",
      "Character =  3\n",
      "Confidence =  4.0146461117316846e-35 %\n",
      "Character =  29\n",
      "Confidence =  2.1694671610430204e-36 %\n",
      "Character =  52\n",
      "Confidence =  0.0 %\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"model.h5\")\n",
    "\n",
    "image = cv2.imread(\"D:\\\\Dataset\\\\test2.png\", cv2.IMREAD_UNCHANGED)\n",
    "\"\"\"if image.shape[2] == 4:\n",
    "    image = read_transparent_png(image)\"\"\"\n",
    "image = clean(image)\n",
    "#cv2.imshow('gray', image)\n",
    "#cv2.waitKey(0)\n",
    "\n",
    "def predict(img):\n",
    "    image_data = img\n",
    "    dataset = np.asarray(image_data)\n",
    "    dataset = dataset.reshape((-1, 32, 32, 1)).astype(np.float32)\n",
    "    print(dataset.shape)\n",
    "    a = model.predict(dataset)[0]\n",
    "\n",
    "    classes = np.genfromtxt('classes.csv', delimiter=',')[:, 0].astype(int)\n",
    "\n",
    "    print(classes)\n",
    "    new = dict(zip(classes, a))\n",
    "    res = sorted(new.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    print(\"#########***#########\")\n",
    "    print(\"Imagefile = \", image)\n",
    "    print(\"Character = \", int(res[0][0]))\n",
    "    print(\"Confidence = \", res[0][1] * 100, \"%\")\n",
    "    if res[0][1] < 1:\n",
    "        print(\"Other predictions\")\n",
    "        for newtemp in res:\n",
    "            print(\"Character = \", newtemp[0])\n",
    "            print(\"Confidence = \", newtemp[1] * 100, \"%\")\n",
    "\n",
    "\n",
    "predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\n"
     ]
    }
   ],
   "source": [
    "print(word[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
